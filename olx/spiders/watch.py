import os, hashlib, json, re, urllib.parse, scrapy
from pathlib import Path
from datetime import datetime, timedelta

SEARCH_URL = os.getenv("SEARCH_URL")
API_BASE   = "https://www.olx.ro/api/v1/offers/"

def get_category_from_url(url: str) -> str:
    """Extrage categoria din URL (canon, nikon, sony, aparat_foto, camera_foto)"""
    url_lower = url.lower()
    if "canon" in url_lower:
        return "canon"
    elif "nikon" in url_lower:
        return "nikon"
    elif "sony" in url_lower:
        return "sony"
    elif "aparat%20foto" in url_lower or "aparat-foto" in url_lower or "aparat foto" in url_lower:
        return "aparat_foto"
    elif "camera%20foto" in url_lower or "camera-foto" in url_lower or "camera foto" in url_lower:
        return "camera_foto"
    else:
        return "unknown"

def build_api_url(src: str, offset=0, limit=40) -> str:
    """TransformÄƒ un URL OLX de cÄƒutare Ã®ntr-un apel API JSON corect (query=â€¦)."""
    parsed = urllib.parse.urlparse(src)
    params = urllib.parse.parse_qs(parsed.query)

    # DacÄƒ keyword-ul e Ã®n path ( /q-ps%20vita/ ), extragem È™i suprascriem
    m = re.search(r"/q-([^/]+)/", parsed.path)
    if m:
        params["query"] = [urllib.parse.unquote_plus(m.group(1))]

    # API-ul nu recunoaÈ™te vechiul â€q", doar â€query"
    if "q" in params and "query" not in params:
        params["query"] = params.pop("q")

    # PÄ‚STRÄ‚M min_id dacÄƒ existÄƒ (pentru a reduce rezultatele la anunÈ›uri noi)
    # min_id este deja Ã®n params dacÄƒ e Ã®n URL-ul original, nu-l È™tergem

    # Paginare
    params["offset"] = [str(offset)]
    params["limit"]  = [str(limit)]

    # Construim URL final
    query = urllib.parse.urlencode({k: v[0] for k, v in params.items()})
    return f"{API_BASE}?{query}"

class WatchJsonSpider(scrapy.Spider):
    name = "watch"
    custom_settings = {
        "ITEM_PIPELINES": {"pipelines.TelegramPipeline": 300},
        "DOWNLOAD_DELAY": 1,
        "RETRY_ENABLED": True,
        "RETRY_TIMES": 3,
        "RETRY_HTTP_CODES": [500, 502, 503, 504, 408, 429],
        "HTTPERROR_ALLOWED_CODES": [429],  # Allow rate limit errors to be retried
        "DEFAULT_REQUEST_HEADERS": {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0 Safari/537.36"
            ),
            "Accept": "application/json",
        },
    }

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # IdentificÄƒ categoria din SEARCH_URL
        self.category = get_category_from_url(SEARCH_URL or "")
        self.logger.info(f"ğŸ” Categoria identificatÄƒ: {self.category}")
        
        # ÃncÄƒrcÄƒm seen IDs pentru categoria respectivÄƒ
        state = Path("state.json")
        if state.exists():
            try:
                data = json.loads(state.read_text())
                # Format nou: dicÈ›ionar cu categorii
                if isinstance(data, dict):
                    category_data = data.get(self.category, [])
                    if isinstance(category_data, list) and len(category_data) > 0:
                        if isinstance(category_data[0], str):
                            # Format vechi: doar ID-uri
                            self.seen = set(category_data)
                        else:
                            # Format nou: listÄƒ de dicÈ›ionare cu ID È™i timestamp
                            self.seen = {item["id"] for item in category_data if isinstance(item, dict) and "id" in item}
                    else:
                        self.seen = set()
                # Compatibilitate: format vechi (listÄƒ simplÄƒ)
                elif isinstance(data, list):
                    if len(data) > 0 and isinstance(data[0], str):
                        self.seen = set(data)
                    else:
                        self.seen = {item["id"] for item in data if isinstance(item, dict) and "id" in item}
                else:
                    self.seen = set()
            except Exception as e:
                self.logger.warning(f"Eroare la Ã®ncÄƒrcarea state.json: {e}")
                self.seen = set()
        else:
            self.seen = set()
        
        self.page_count = 0  # Contor pentru pagini
        self.max_pages = 1  # Maxim 1 paginÄƒ (optimizare)
        self.consecutive_seen = 0  # Contor pentru anunÈ›uri consecutive deja vÄƒzute
        self.max_consecutive_seen = 10  # OpreÈ™te dacÄƒ 10 consecutive sunt deja vÄƒzute
        
        # Filtrare dupÄƒ data publicÄƒrii: doar anunÈ›uri din ultimele 30 de minute
        self.min_time = datetime.now() - timedelta(minutes=30)

    def start_requests(self):
        # ResetÄƒm contoarele la Ã®nceputul fiecÄƒrei cÄƒutÄƒri
        self.page_count = 0
        self.consecutive_seen = 0
        yield scrapy.Request(
            build_api_url(SEARCH_URL, offset=0, limit=40),
            callback=self.parse_api,
            meta={"page": 1}
        )

    def parse_api(self, response):
        self.page_count += 1
        
        # VerificÄƒ dacÄƒ am depÄƒÈ™it limita de pagini
        if self.page_count > self.max_pages:
            self.logger.info(f"LimitÄƒ de {self.max_pages} paginÄƒ atinsÄƒ. OpreÈ™te paginarea.")
            return

        # VerificÄƒ status code
        if response.status != 200:
            self.logger.warning(f"Status code {response.status} pentru {response.url}")
            # Retry automat dacÄƒ e configurat
            return

        try:
            data = json.loads(response.text)
        except json.JSONDecodeError as e:
            self.logger.error(f"Failed to parse OLX JSON: {e}. Response: {response.text[:200]}")
            return
        except Exception as e:
            self.logger.error(f"Unexpected error parsing response: {e}")
            return

        items_in_page = 0
        new_items = 0
        skipped_old = 0  # Contor pentru anunÈ›uri prea vechi
        
        for offer in data.get("data", []):
            uid = str(offer.get("id"))
            title = offer.get("title", "").strip()
            link = offer.get("url")
            price = (
                offer["price"]["value"]["display"]
                if offer.get("price") and offer["price"].get("value")
                else None
            )
            
            if uid and title and link:
                items_in_page += 1
                
                # VerificÄƒ data publicÄƒrii anunÈ›ului
                offer_time = None
                # ÃncearcÄƒ sÄƒ extragÄƒ data din diferite cÃ¢mpuri posibile
                for date_field in ["created_time", "created_at", "date", "published_at", "last_refresh_time"]:
                    if offer.get(date_field):
                        try:
                            # Poate fi timestamp (int) sau string ISO
                            timestamp = offer[date_field]
                            if isinstance(timestamp, (int, float)):
                                offer_time = datetime.fromtimestamp(timestamp / 1000 if timestamp > 1e10 else timestamp)
                            elif isinstance(timestamp, str):
                                # ÃncearcÄƒ sÄƒ parseze diferite formate
                                for fmt in ["%Y-%m-%dT%H:%M:%S", "%Y-%m-%dT%H:%M:%S.%f", "%Y-%m-%d %H:%M:%S"]:
                                    try:
                                        offer_time = datetime.strptime(timestamp.split("+")[0].split("Z")[0], fmt)
                                        break
                                    except:
                                        continue
                            if offer_time:
                                break
                        except Exception as e:
                            self.logger.debug(f"Failed to parse date field {date_field}: {e}")
                            continue
                
                # DacÄƒ nu am gÄƒsit data, logÄƒm un warning dar permitem anunÈ›ul (pentru a nu pierde anunÈ›uri valide)
                if not offer_time:
                    self.logger.warning(f"AnunÈ› {uid}: nu s-a putut determina data publicÄƒrii. CÃ¢mpuri disponibile: {list(offer.keys())[:10]}")
                    # Permitem anunÈ›ul dacÄƒ nu putem determina data (pentru siguranÈ›Äƒ)
                elif offer_time < self.min_time:
                    # AnunÈ›ul e prea vechi, Ã®l ignorÄƒm
                    skipped_old += 1
                    self.logger.debug(f"AnunÈ› {uid} ignorat: prea vechi (data: {offer_time}, minim: {self.min_time})")
                    continue
                
                # VerificÄƒ dacÄƒ e deja vÄƒzut
                if uid in self.seen:
                    self.consecutive_seen += 1
                    self.logger.debug(f"AnunÈ› {uid} deja vÄƒzut. Consecutive seen: {self.consecutive_seen}")
                    
                    # DacÄƒ 10 consecutive sunt deja vÄƒzute, opreÈ™te
                    if self.consecutive_seen >= self.max_consecutive_seen:
                        self.logger.info(
                            f"OpreÈ™te paginarea: {self.consecutive_seen} anunÈ›uri consecutive "
                            f"deja vÄƒzute (limitÄƒ: {self.max_consecutive_seen})"
                        )
                        return
                else:
                    # ResetÄƒm contorul cÃ¢nd gÄƒsim unul nou
                    self.consecutive_seen = 0
                    new_items += 1
                    yield {
                        "id": uid, 
                        "title": title, 
                        "price": price, 
                        "link": link, 
                        "created_time": offer_time.isoformat(),
                        "category": self.category  # AdÄƒugÄƒm categoria pentru pipeline
                    }
                    # AdÄƒugÄƒm imediat Ã®n seen pentru a evita duplicatele Ã®n aceeaÈ™i sesiune
                    self.seen.add(uid)

        self.logger.info(
            f"Pagina {self.page_count}: {items_in_page} anunÈ›uri procesate, "
            f"{new_items} noi (din ultimele 30 min), {items_in_page - new_items - skipped_old} deja vÄƒzute, "
            f"{skipped_old} prea vechi (ignorate), timp minim: {self.min_time.strftime('%Y-%m-%d %H:%M:%S')}"
        )

        # VerificÄƒ dacÄƒ trebuie sÄƒ continuÄƒm paginarea
        if self.consecutive_seen >= self.max_consecutive_seen:
            self.logger.info("OpreÈ™te paginarea: prea multe anunÈ›uri consecutive deja vÄƒzute")
            return

        # Pagina urmÄƒtoare (doar dacÄƒ nu am atins limita)
        next_link = data.get("links", {}).get("next")
        if next_link and self.page_count < self.max_pages:
            yield scrapy.Request(
                next_link,
                callback=self.parse_api,
                meta={"page": self.page_count + 1}
            )
