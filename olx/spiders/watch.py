import os, json, re, urllib.parse, scrapy
from pathlib import Path
from datetime import datetime, timedelta

SEARCH_URL = os.getenv("SEARCH_URL")
API_BASE   = "https://www.olx.ro/api/v1/offers/"

def get_category_from_url(url: str) -> str:
    """Extrage categoria din URL (canon, nikon, sony, aparat_foto, camera_foto)"""
    url_lower = url.lower()
    if "canon" in url_lower:
        return "canon"
    elif "nikon" in url_lower:
        return "nikon"
    elif "sony" in url_lower:
        return "sony"
    elif "aparat%20foto" in url_lower or "aparat-foto" in url_lower or "aparat foto" in url_lower:
        return "aparat_foto"
    elif "camera%20foto" in url_lower or "camera-foto" in url_lower or "camera foto" in url_lower:
        return "camera_foto"
    else:
        return "unknown"

def build_api_url(src: str, offset=0, limit=40) -> str:
    """TransformÄƒ un URL OLX de cÄƒutare Ã®ntr-un apel API JSON corect (query=â€¦)."""
    parsed = urllib.parse.urlparse(src)
    params = urllib.parse.parse_qs(parsed.query)

    # DacÄƒ keyword-ul e Ã®n path ( /q-ps%20vita/ ), extragem È™i suprascriem
    m = re.search(r"/q-([^/]+)/", parsed.path)
    if m:
        params["query"] = [urllib.parse.unquote_plus(m.group(1))]

    # API-ul nu recunoaÈ™te vechiul â€q", doar â€query"
    if "q" in params and "query" not in params:
        params["query"] = params.pop("q")

    # Paginare
    params["offset"] = [str(offset)]
    params["limit"]  = [str(limit)]

    # Construim URL final
    query = urllib.parse.urlencode({k: v[0] for k, v in params.items()})
    return f"{API_BASE}?{query}"

def try_parse_date(value):
    """ÃncearcÄƒ sÄƒ parseze o valoare ca datÄƒ/timestamp"""
    if value is None:
        return None
    
    try:
        # Timestamp numeric (milisecunde sau secunde)
        if isinstance(value, (int, float)):
            return datetime.fromtimestamp(value / 1000 if value > 1e10 else value)
        
        # String ISO format
        if isinstance(value, str):
            # EliminÄƒ timezone info pentru parsing
            clean_value = value.split("+")[0].split("Z")[0].split(".")[0]
            
            # Formate comune
            formats = [
                "%Y-%m-%dT%H:%M:%S",
                "%Y-%m-%dT%H:%M:%S.%f",
                "%Y-%m-%d %H:%M:%S",
                "%Y-%m-%d %H:%M:%S.%f",
                "%Y-%m-%d",
                "%d/%m/%Y %H:%M:%S",
                "%d-%m-%Y %H:%M:%S",
            ]
            
            for fmt in formats:
                try:
                    return datetime.strptime(clean_value, fmt)
                except ValueError:
                    continue
    except Exception:
        pass
    
    return None

def find_date_in_offer(offer, depth=0, max_depth=3):
    """CautÄƒ recursiv data Ã®n offer È™i sub-obiecte (case-insensitive)"""
    if depth > max_depth or not isinstance(offer, dict):
        return None
    
    # ListÄƒ de cÃ¢mpuri posibile (case-insensitive)
    date_keywords = ["created", "date", "published", "timestamp", "time", "refresh", "updated"]
    
    # CautÄƒ direct Ã®n cheile din offer
    for key, value in offer.items():
        if value is None:
            continue
            
        key_lower = str(key).lower()
        
        # VerificÄƒ dacÄƒ cheia conÈ›ine un keyword de datÄƒ
        if any(keyword in key_lower for keyword in date_keywords):
            parsed_date = try_parse_date(value)
            if parsed_date:
                return parsed_date
        
        # DacÄƒ valoarea e un dicÈ›ionar, cautÄƒ recursiv
        if isinstance(value, dict):
            nested_date = find_date_in_offer(value, depth + 1, max_depth)
            if nested_date:
                return nested_date
        
        # DacÄƒ valoarea e o listÄƒ de dicÈ›ionare, cautÄƒ Ã®n ele
        elif isinstance(value, list) and len(value) > 0 and isinstance(value[0], dict):
            for item in value[:2]:  # VerificÄƒ doar primele 2 elemente
                nested_date = find_date_in_offer(item, depth + 1, max_depth)
                if nested_date:
                    return nested_date
    
    return None

class WatchJsonSpider(scrapy.Spider):
    name = "watch"
    custom_settings = {
        "ITEM_PIPELINES": {"pipelines.TelegramPipeline": 300},
        "DOWNLOAD_DELAY": 1,
        "RETRY_ENABLED": True,
        "RETRY_TIMES": 3,
        "RETRY_HTTP_CODES": [500, 502, 503, 504, 408, 429],
        "HTTPERROR_ALLOWED_CODES": [429],  # Allow rate limit errors to be retried
        "DEFAULT_REQUEST_HEADERS": {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0 Safari/537.36"
            ),
            "Accept": "application/json",
        },
    }

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # IdentificÄƒ categoria din SEARCH_URL
        self.category = get_category_from_url(SEARCH_URL or "")
        self.logger.info(f"ğŸ” Categoria identificatÄƒ: {self.category}")
        
        if self.category == "unknown":
            self.logger.warning(f"âš ï¸ SEARCH_URL nu conÈ›ine categorie cunoscutÄƒ: {SEARCH_URL or 'None'}")
        
        # ÃncÄƒrcÄƒm seen IDs pentru categoria respectivÄƒ
        state = Path("state.json")
        if state.exists():
            try:
                data = json.loads(state.read_text())
                # Format nou: dicÈ›ionar cu categorii
                if isinstance(data, dict):
                    category_data = data.get(self.category, [])
                    if isinstance(category_data, list) and len(category_data) > 0:
                        if isinstance(category_data[0], str):
                            # Format vechi: doar ID-uri
                            self.seen = set(category_data)
                        else:
                            # Format nou: listÄƒ de dicÈ›ionare cu ID È™i timestamp
                            self.seen = {item["id"] for item in category_data if isinstance(item, dict) and "id" in item}
                    else:
                        self.seen = set()
                # Compatibilitate: format vechi (listÄƒ simplÄƒ)
                elif isinstance(data, list):
                    if len(data) > 0 and isinstance(data[0], str):
                        self.seen = set(data)
                    else:
                        self.seen = {item["id"] for item in data if isinstance(item, dict) and "id" in item}
                else:
                    self.seen = set()
            except Exception as e:
                self.logger.warning(f"Eroare la Ã®ncÄƒrcarea state.json: {e}")
                self.seen = set()
        else:
            self.seen = set()
        
        self.page_count = 0  # Contor pentru pagini
        self.max_pages = 2  # Maxim 2 pagini (80 anunÈ›uri)
        self.consecutive_seen = 0  # Contor pentru anunÈ›uri consecutive deja vÄƒzute
        self.max_consecutive_seen = 30  # OpreÈ™te dacÄƒ 30 consecutive sunt deja vÄƒzute
        
        # Filtrare dupÄƒ data publicÄƒrii: doar anunÈ›uri din ultimele 4 ore
        self.min_time = datetime.now() - timedelta(hours=4)

    def start_requests(self):
        # ResetÄƒm contoarele la Ã®nceputul fiecÄƒrei cÄƒutÄƒri
        self.page_count = 0
        self.consecutive_seen = 0
        yield scrapy.Request(
            build_api_url(SEARCH_URL, offset=0, limit=40),
            callback=self.parse_api,
            meta={"page": 1}
        )

    def parse_api(self, response):
        self.page_count += 1
        
        # VerificÄƒ dacÄƒ am depÄƒÈ™it limita de pagini
        if self.page_count > self.max_pages:
            self.logger.info(f"LimitÄƒ de {self.max_pages} paginÄƒ atinsÄƒ. OpreÈ™te paginarea.")
            return

        # VerificÄƒ status code
        if response.status != 200:
            self.logger.warning(f"Status code {response.status} pentru {response.url}")
            # Retry automat dacÄƒ e configurat
            return

        try:
            data = json.loads(response.text)
        except json.JSONDecodeError as e:
            self.logger.error(f"Failed to parse OLX JSON: {e}. Response: {response.text[:200]}")
            return
        except Exception as e:
            self.logger.error(f"Unexpected error parsing response: {e}")
            return

        items_in_page = 0
        new_items = 0
        skipped_old = 0  # Contor pentru anunÈ›uri prea vechi
        no_date_count = 0  # Contor pentru anunÈ›uri fÄƒrÄƒ datÄƒ
        new_ids = []  # ListÄƒ cu ID-urile anunÈ›urilor noi
        
        for offer in data.get("data", []):
            uid = str(offer.get("id"))
            title = offer.get("title", "").strip()
            link = offer.get("url")
            price = (
                offer["price"]["value"]["display"]
                if offer.get("price") and offer["price"].get("value")
                else None
            )
            
            if uid and title and link:
                items_in_page += 1
                
                # Extragere datÄƒ Ã®mbunÄƒtÄƒÈ›itÄƒ (case-insensitive, nested, camelCase)
                offer_time = find_date_in_offer(offer)
                
                # DacÄƒ nu am gÄƒsit data, logÄƒm un warning dar permitem anunÈ›ul (pentru a nu pierde anunÈ›uri valide)
                if not offer_time:
                    no_date_count += 1
                    self.logger.warning(f"AnunÈ› {uid}: nu s-a putut determina data publicÄƒrii. CÃ¢mpuri disponibile: {list(offer.keys())[:15]}")
                    # Permitem anunÈ›ul dacÄƒ nu putem determina data (pentru siguranÈ›Äƒ)
                elif offer_time < self.min_time:
                    # AnunÈ›ul e prea vechi, Ã®l ignorÄƒm
                    skipped_old += 1
                    self.logger.debug(f"AnunÈ› {uid} ignorat: prea vechi (data: {offer_time.strftime('%Y-%m-%d %H:%M')}, minim: {self.min_time.strftime('%Y-%m-%d %H:%M')})")
                    continue
                
                # VerificÄƒ dacÄƒ e deja vÄƒzut
                if uid in self.seen:
                    self.consecutive_seen += 1
                    self.logger.debug(f"AnunÈ› {uid} deja vÄƒzut. Consecutive seen: {self.consecutive_seen}")
                    
                    # DacÄƒ 10 consecutive sunt deja vÄƒzute, opreÈ™te
                    if self.consecutive_seen >= self.max_consecutive_seen:
                        self.logger.info(
                            f"OpreÈ™te paginarea: {self.consecutive_seen} anunÈ›uri consecutive "
                            f"deja vÄƒzute (limitÄƒ: {self.max_consecutive_seen})"
                        )
                        return
                else:
                    # ResetÄƒm contorul cÃ¢nd gÄƒsim unul nou
                    self.consecutive_seen = 0
                    new_items += 1
                    new_ids.append(uid)
                    yield {
                        "id": uid, 
                        "title": title, 
                        "price": price, 
                        "link": link, 
                        "created_time": offer_time.isoformat() if offer_time else datetime.now().isoformat(),
                        "category": self.category  # AdÄƒugÄƒm categoria pentru pipeline
                    }
                    # AdÄƒugÄƒm imediat Ã®n seen pentru a evita duplicatele Ã®n aceeaÈ™i sesiune
                    self.seen.add(uid)

        # Logging Ã®mbunÄƒtÄƒÈ›it cu statistici detaliate
        new_ids_preview = new_ids[:5] if new_ids else []
        self.logger.info(
            f"[{self.category.upper()}] Pagina {self.page_count}: {items_in_page} anunÈ›uri procesate\n"
            f"  âœ… {new_items} noi gÄƒsite" + (f" (primele: {new_ids_preview})" if new_ids_preview else "") + "\n"
            f"  â­ï¸ {items_in_page - new_items - skipped_old - no_date_count} deja vÄƒzute (seen set: {len(self.seen)} anunÈ›uri)\n"
            f"  â° {skipped_old} prea vechi (ignorate, minim: {self.min_time.strftime('%Y-%m-%d %H:%M')})\n"
            f"  âš ï¸ {no_date_count} fÄƒrÄƒ datÄƒ (procesate pentru siguranÈ›Äƒ)\n"
            f"  ğŸ“Š Consecutive seen: {self.consecutive_seen}/{self.max_consecutive_seen}"
        )

        # VerificÄƒ dacÄƒ trebuie sÄƒ continuÄƒm paginarea
        if self.consecutive_seen >= self.max_consecutive_seen:
            self.logger.info("OpreÈ™te paginarea: prea multe anunÈ›uri consecutive deja vÄƒzute")
            return

        # Pagina urmÄƒtoare (doar dacÄƒ nu am atins limita)
        next_link = data.get("links", {}).get("next")
        if next_link and self.page_count < self.max_pages:
            # Extrage URL-ul dacÄƒ next_link e dicÈ›ionar
            if isinstance(next_link, dict):
                next_url = next_link.get("href") or next_link.get("url") or next_link.get("link")
            else:
                next_url = next_link
            
            if next_url:
                yield scrapy.Request(
                    next_url,
                    callback=self.parse_api,
                    meta={"page": self.page_count + 1}
                )
